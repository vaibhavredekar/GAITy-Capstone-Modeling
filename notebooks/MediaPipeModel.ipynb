{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72c3a508",
   "metadata": {},
   "source": [
    "# Visualization utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6e99b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@markdown To better demonstrate the Pose Landmarker API, we have created a set of visualization tools that will be used in this colab. These will draw the landmarks on a detect person, as well as the expected connections between those markers.\n",
    "\n",
    "from mediapipe import solutions\n",
    "from mediapipe.framework.formats import landmark_pb2\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# STEP 4: Draw landmarks on image\n",
    "# ----------------------------\n",
    "def draw_landmarks_on_image(rgb_image, detection_result):\n",
    "    \"\"\"\n",
    "    rgb_image: NumPy array (RGB)\n",
    "    detection_result: PoseLandmarkerResult from detector.detect()\n",
    "    Returns: annotated_image (RGB NumPy array) ready for Pillow display\n",
    "    \"\"\"\n",
    "    # Convert RGB -> BGR for drawing\n",
    "    annotated_image = cv2.cvtColor(np.copy(rgb_image), cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    if detection_result.pose_landmarks:\n",
    "        for pose_landmarks_list in detection_result.pose_landmarks:\n",
    "            # Wrap the list of NormalizedLandmark into NormalizedLandmarkList proto\n",
    "            pose_proto = landmark_pb2.NormalizedLandmarkList()\n",
    "            pose_proto.landmark.extend([\n",
    "                landmark_pb2.NormalizedLandmark(x=l.x, y=l.y, z=l.z)\n",
    "                for l in pose_landmarks_list\n",
    "            ])\n",
    "            # Draw landmarks\n",
    "            mp_drawing.draw_landmarks(\n",
    "                annotated_image,\n",
    "                pose_proto,\n",
    "                mp_pose.POSE_CONNECTIONS,\n",
    "                landmark_drawing_spec=mp_drawing.DrawingSpec(color=(0, 255, 0), thickness=2, circle_radius=2),\n",
    "                connection_drawing_spec=mp_drawing.DrawingSpec(color=(0, 0, 255), thickness=2)\n",
    "            )\n",
    "\n",
    "    # Convert back to RGB for Pillow\n",
    "    annotated_image = cv2.cvtColor(annotated_image, cv2.COLOR_BGR2RGB)\n",
    "    return annotated_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5fa16d2",
   "metadata": {},
   "source": [
    "## Download test image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e05de724",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import sys\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "# ----------------------------\n",
    "# 1. Download the image\n",
    "# ----------------------------\n",
    "url = \"https://cdn.pixabay.com/photo/2019/03/12/20/39/girl-4051811_960_720.jpg\"\n",
    "image_path = \"image.jpg\"\n",
    "\n",
    "if not os.path.exists(image_path):\n",
    "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "    req = urllib.request.Request(url, headers=headers)\n",
    "    with urllib.request.urlopen(req) as response, open(image_path, 'wb') as out_file:\n",
    "        out_file.write(response.read())\n",
    "\n",
    "# ----------------------------\n",
    "# 2. Open the image with Pillow\n",
    "# ----------------------------\n",
    "img = Image.open(image_path)\n",
    "\n",
    "# ----------------------------\n",
    "# 3. Display the image\n",
    "# ----------------------------\n",
    "IN_COLAB = \"google.colab\" in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    # If in Colab, use cv2_imshow to display inline\n",
    "    import cv2\n",
    "    import numpy as np\n",
    "    from google.colab.patches import cv2_imshow\n",
    "\n",
    "    img_cv = cv2.cvtColor(np.array(img), cv2.COLOR_RGB2BGR)\n",
    "    cv2_imshow(img_cv)\n",
    "else:\n",
    "    # On local machine, open in default image viewer (separate window)\n",
    "    img.show()  # fully interactive and closable window\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cfb13c3",
   "metadata": {},
   "source": [
    "\n",
    "Optionally, you can upload your own image. If you want to do so, uncomment and run the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0017d6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.colab import files\n",
    "# uploaded = files.upload()\n",
    "\n",
    "# for filename in uploaded:\n",
    "#   content = uploaded[filename]\n",
    "#   with open(filename, 'wb') as f:\n",
    "#     f.write(content)\n",
    "\n",
    "# if len(uploaded.keys()):\n",
    "#   IMAGE_FILE = next(iter(uploaded))\n",
    "#   print('Uploaded file:', IMAGE_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8d36b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# ----------------------------\\n# 1. Detect environment\\n# ----------------------------\\nIN_COLAB = \"google.colab\" in sys.modules\\n\\nif IN_COLAB:\\n    # Colab file upload\\n    from google.colab import files\\n    uploaded = files.upload()\\n\\n    IMAGE_FILE = None\\n    if len(uploaded.keys()):\\n        IMAGE_FILE = next(iter(uploaded))\\n        # Save file locally\\n        with open(IMAGE_FILE, \\'wb\\') as f:\\n            f.write(uploaded[IMAGE_FILE])\\n        print(\\'Uploaded file:\\', IMAGE_FILE)\\n\\nelse:\\n    # Local Python / VS Code\\n    # Ask user to provide a path or choose file interactively\\n    IMAGE_FILE = input(\"Enter path to image file: \").strip()\\n    if not os.path.exists(IMAGE_FILE):\\n        raise FileNotFoundError(f\"File not found: {IMAGE_FILE}\")\\n    print(\\'Using local file:\\', IMAGE_FILE)\\n\\n# ----------------------------\\n# 2. Read and display the image\\n# ----------------------------\\n\\nimg = cv2.imread(IMAGE_FILE)\\n\\nif IN_COLAB:\\n    from google.colab.patches import cv2_imshow\\n    cv2_imshow(img)\\nelse:\\n    cv2.imshow(\"Uploaded Image\", img)\\n    cv2.waitKey(3000)  # window auto-closes after 3 seconds\\n    cv2.destroyAllWindows()'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''import sys\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "# ----------------------------\n",
    "# 1. Detect environment\n",
    "# ----------------------------\n",
    "IN_COLAB = \"google.colab\" in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    # Colab file upload\n",
    "    from google.colab import files\n",
    "    uploaded = files.upload()\n",
    "\n",
    "    IMAGE_FILE = None\n",
    "    if len(uploaded.keys()):\n",
    "        IMAGE_FILE = next(iter(uploaded))\n",
    "        # Save file locally\n",
    "        with open(IMAGE_FILE, 'wb') as f:\n",
    "            f.write(uploaded[IMAGE_FILE])\n",
    "        print('Uploaded file:', IMAGE_FILE)\n",
    "\n",
    "else:\n",
    "    # Local Python / VS Code\n",
    "    # Ask user to provide a path or choose file interactively\n",
    "    IMAGE_FILE = input(\"Enter path to image file: \").strip()\n",
    "    if not os.path.exists(IMAGE_FILE):\n",
    "        raise FileNotFoundError(f\"File not found: {IMAGE_FILE}\")\n",
    "    print('Using local file:', IMAGE_FILE)\n",
    "\n",
    "# ----------------------------\n",
    "# 2. Read and display the image\n",
    "# ----------------------------\n",
    "# Open the image using Pillow\n",
    "img = Image.open(IMAGE_FILE)\n",
    "\n",
    "if IN_COLAB:\n",
    "    # Convert Pillow image to OpenCV format for inline display in Colab\n",
    "    import cv2\n",
    "    import numpy as np\n",
    "    img_cv = cv2.cvtColor(np.array(img), cv2.COLOR_RGB2BGR)\n",
    "    from google.colab.patches import cv2_imshow\n",
    "    cv2_imshow(img_cv)\n",
    "else:\n",
    "    # Local machine: open in a separate, closable window\n",
    "    img.show()  # This opens the image in the default image viewer\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e818f3",
   "metadata": {},
   "source": [
    "## Running inference and visualizing the results\n",
    "The final step is to run pose landmark detection on your selected image. This involves creating your PoseLandmarker object, loading your image, running detection, and finally, the optional step of displaying the image with visualizations.\n",
    "\n",
    "Check out the [MediaPipe documentation](https://developers.google.com/mediapipe/solutions/vision/pose_landmarker/python) to learn more about configuration options that this solution supports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fe44a86f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1765968841.104998  162395 gl_context.cc:369] GL version: 2.1 (2.1 Metal - 90.5), renderer: Apple M1 Max\n",
      "W0000 00:00:1765968841.181968  172436 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1765968841.222268  172443 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    }
   ],
   "source": [
    "# STEP 0: Imports\n",
    "import mediapipe as mp\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n",
    "import sys\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import cv2  # needed only for Colab inline display and BGR<->RGB conversion\n",
    "\n",
    "# Detect environment\n",
    "IN_COLAB = \"google.colab\" in sys.modules\n",
    "if IN_COLAB:\n",
    "    from google.colab.patches import cv2_imshow\n",
    "\n",
    "# STEP 0.1: Drawing utils\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_pose = mp.solutions.pose\n",
    "from mediapipe.framework.formats import landmark_pb2\n",
    "\n",
    "# ----------------------------\n",
    "# STEP 1: Create PoseLandmarker\n",
    "# ----------------------------\n",
    "model_path = '../models/pose_landmarker_heavy.task'\n",
    "base_options = python.BaseOptions(model_asset_path=model_path)\n",
    "options = vision.PoseLandmarkerOptions(\n",
    "    base_options=base_options,\n",
    "    output_segmentation_masks=True\n",
    ")\n",
    "detector = vision.PoseLandmarker.create_from_options(options)\n",
    "\n",
    "# ----------------------------\n",
    "# STEP 2: Load input image\n",
    "# ----------------------------\n",
    "image_path = \"image.jpg\"\n",
    "image = mp.Image.create_from_file(image_path)\n",
    "image_np = image.numpy_view()  # RGB NumPy array\n",
    "\n",
    "# ----------------------------\n",
    "# STEP 3: Detect pose landmarks\n",
    "# ----------------------------\n",
    "detection_result = detector.detect(image)\n",
    "\n",
    "annotated_image = draw_landmarks_on_image(image_np, detection_result)\n",
    "\n",
    "# ----------------------------\n",
    "# STEP 5: Display annotated image\n",
    "# ----------------------------\n",
    "if IN_COLAB:\n",
    "    cv2_imshow(cv2.cvtColor(annotated_image, cv2.COLOR_RGB2BGR))\n",
    "else:\n",
    "    pil_img = Image.fromarray(annotated_image)\n",
    "    pil_img.show()  # fully interactive, closable window\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f13f7ef9",
   "metadata": {},
   "source": [
    "Visualize the pose segmentation mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bca5b802",
   "metadata": {},
   "outputs": [],
   "source": [
    " # ----------------------------\n",
    "# STEP 6: Display segmentation mask (optional)\n",
    "# ----------------------------\n",
    "if detection_result.segmentation_masks:\n",
    "    mask = detection_result.segmentation_masks[0].numpy_view()  # HxW array\n",
    "    # Convert mask to 3-channel RGB\n",
    "    mask_rgb = np.repeat(mask[:, :, np.newaxis], 3, axis=2) * 255\n",
    "    mask_rgb = mask_rgb.astype(np.uint8)\n",
    "\n",
    "    if IN_COLAB:\n",
    "        cv2_imshow(mask_rgb)\n",
    "    else:\n",
    "        pil_mask = Image.fromarray(mask_rgb)\n",
    "        pil_mask.show()  # separate, closable window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5dd85c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "pose_landmarks_list = detection_result.pose_landmarks\n",
    "\n",
    "with open(\"../data/pose_landmarks.csv\", \"w\", newline=\"\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"pose_idx\", \"landmark_idx\", \"x\", \"y\", \"z\"])\n",
    "    for pose_idx, pose_landmarks in enumerate(pose_landmarks_list):\n",
    "        for lm_idx, lm in enumerate(pose_landmarks):\n",
    "            writer.writerow([pose_idx, lm_idx, lm.x, lm.y, lm.z])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "022de9d0",
   "metadata": {},
   "source": [
    "# For Videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fde7677b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1766145571.903250  111825 gl_context.cc:369] GL version: 2.1 (2.1 Metal - 90.5), renderer: Apple M1 Max\n",
      "W0000 00:00:1766145571.987698  121126 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1766145572.019666  121126 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1766145572.121951  121133 landmark_projection_calculator.cc:186] Using NORM_RECT without IMAGE_DIMENSIONS is only supported for the square ROI. Provide IMAGE_DIMENSIONS or use PROJECTION_MATRIX.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ñ∂ Processing 205 frames...\n",
      "‚è≥ 180/205 (87.8%)\n",
      "‚úÖ Done!\n",
      "üé¨ Video: ../data/annotated_videos/annotated_video.mp4\n",
      "üìä CSV:   ../data/csv/pose_landmarks2.csv\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------------\n",
    "# MediaPipe PoseLandmarker\n",
    "# Export pose landmarks to CSV + annotated video (macOS)\n",
    "# -------------------------------------------------------\n",
    "\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n",
    "import numpy as np\n",
    "from mediapipe.framework.formats import landmark_pb2\n",
    "import csv\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "# ----------------------------\n",
    "# CONFIG\n",
    "# ----------------------------\n",
    "VIDEO_PATH = \"../data/videos/video.mp4\"\n",
    "MODEL_PATH = \"../models/pose_landmarker_heavy.task\"\n",
    "OUT_VIDEO = \"../data/annotated_videos/annotated_video.mp4\"\n",
    "OUT_CSV = \"../data/csv/pose_landmarks2.csv\"\n",
    "\n",
    "# ----------------------------\n",
    "# STEP 1: PoseLandmarker\n",
    "# ----------------------------\n",
    "base_options = python.BaseOptions(model_asset_path=MODEL_PATH)\n",
    "options = vision.PoseLandmarkerOptions(\n",
    "    base_options=base_options,\n",
    "    running_mode=vision.RunningMode.VIDEO,\n",
    "    output_segmentation_masks=False\n",
    ")\n",
    "detector = vision.PoseLandmarker.create_from_options(options)\n",
    "\n",
    "# ----------------------------\n",
    "# STEP 2: Video I/O\n",
    "# ----------------------------\n",
    "cap = cv2.VideoCapture(VIDEO_PATH)\n",
    "if not cap.isOpened():\n",
    "    print(\"‚ùå Could not open video.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "width  = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fps    = cap.get(cv2.CAP_PROP_FPS)\n",
    "total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "writer = cv2.VideoWriter(OUT_VIDEO, fourcc, fps, (width, height))\n",
    "\n",
    "# ----------------------------\n",
    "# STEP 3: CSV writer\n",
    "# ----------------------------\n",
    "csv_file = open(OUT_CSV, \"w\", newline=\"\")\n",
    "csv_writer = csv.writer(csv_file)\n",
    "\n",
    "csv_writer.writerow([\n",
    "    \"frame\",\n",
    "    \"timestamp_ms\",\n",
    "    \"landmark_id\",\n",
    "    \"x_norm\",\n",
    "    \"y_norm\",\n",
    "    \"z_norm\",\n",
    "    \"visibility\",\n",
    "    \"x_px\",\n",
    "    \"y_px\"\n",
    "])\n",
    "\n",
    "print(f\"‚ñ∂ Processing {total_frames} frames...\")\n",
    "\n",
    "# ----------------------------\n",
    "# STEP 4: Frame loop\n",
    "# ----------------------------\n",
    "frame_idx = 0\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # BGR ‚Üí RGB\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=frame_rgb)\n",
    "\n",
    "    timestamp_ms = int((frame_idx / fps) * 1000)\n",
    "\n",
    "    result = detector.detect_for_video(mp_image, timestamp_ms)\n",
    "\n",
    "    annotated_rgb = np.copy(frame_rgb)\n",
    "\n",
    "    if result.pose_landmarks:\n",
    "        pose_landmarks = result.pose_landmarks[0]  # first person\n",
    "\n",
    "        # ----- EXPORT LANDMARKS -----\n",
    "        for lm_id, lm in enumerate(pose_landmarks):\n",
    "            x_px = int(lm.x * width)\n",
    "            y_px = int(lm.y * height)\n",
    "\n",
    "            csv_writer.writerow([\n",
    "                frame_idx,\n",
    "                timestamp_ms,\n",
    "                lm_id,\n",
    "                lm.x,\n",
    "                lm.y,\n",
    "                lm.z,\n",
    "                lm.visibility,\n",
    "                x_px,\n",
    "                y_px\n",
    "            ])\n",
    "\n",
    "        # ----- DRAW LANDMARKS -----\n",
    "        pose_proto = landmark_pb2.NormalizedLandmarkList()\n",
    "        pose_proto.landmark.extend([\n",
    "            landmark_pb2.NormalizedLandmark(\n",
    "                x=lm.x, y=lm.y, z=lm.z\n",
    "            ) for lm in pose_landmarks\n",
    "        ])\n",
    "\n",
    "        mp.solutions.drawing_utils.draw_landmarks(\n",
    "            annotated_rgb,\n",
    "            pose_proto,\n",
    "            mp.solutions.pose.POSE_CONNECTIONS,\n",
    "            landmark_drawing_spec=mp.solutions.drawing_utils.DrawingSpec(\n",
    "                color=(0, 255, 0), thickness=2, circle_radius=2\n",
    "            ),\n",
    "            connection_drawing_spec=mp.solutions.drawing_utils.DrawingSpec(\n",
    "                color=(255, 0, 0), thickness=2\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # RGB ‚Üí BGR\n",
    "    annotated_bgr = cv2.cvtColor(annotated_rgb, cv2.COLOR_RGB2BGR)\n",
    "    writer.write(annotated_bgr)\n",
    "\n",
    "    # Progress\n",
    "    if frame_idx % 30 == 0:\n",
    "        pct = 100 * frame_idx / total_frames\n",
    "        print(f\"\\r‚è≥ {frame_idx}/{total_frames} ({pct:.1f}%)\", end=\"\")\n",
    "\n",
    "    frame_idx += 1\n",
    "\n",
    "# ----------------------------\n",
    "# STEP 5: Cleanup\n",
    "# ----------------------------\n",
    "cap.release()\n",
    "writer.release()\n",
    "csv_file.close()\n",
    "\n",
    "print(\"\\n‚úÖ Done!\")\n",
    "print(f\"üé¨ Video: {OUT_VIDEO}\")\n",
    "print(f\"üìä CSV:   {OUT_CSV}\")\n",
    "\n",
    "# ----------------------------\n",
    "# STEP 6: Open in QuickTime\n",
    "# ----------------------------\n",
    "try:\n",
    "    subprocess.run([\"open\", OUT_VIDEO])\n",
    "except Exception:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1732956",
   "metadata": {},
   "source": [
    "# use with a link "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4cd548",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------\n",
    "# MediaPipe PoseLandmarker\n",
    "# Process video directly from a YouTube / online link\n",
    "# -------------------------------------------------------\n",
    "\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n",
    "import numpy as np\n",
    "from mediapipe.framework.formats import landmark_pb2\n",
    "import csv\n",
    "import sys\n",
    "import subprocess\n",
    "import json\n",
    "\n",
    "# ----------------------------\n",
    "# CONFIG\n",
    "# ----------------------------\n",
    "VIDEO_URL = \"https://www.youtube.com/watch?v=XXXXXX\"\n",
    "MODEL_PATH = \"../models/pose_landmarker_heavy.task\"\n",
    "OUT_VIDEO = \"annotated_video.mp4\"\n",
    "OUT_CSV = \"pose_landmarks.csv\"\n",
    "\n",
    "# ----------------------------\n",
    "# STEP 0: Resolve direct video stream URL\n",
    "# ----------------------------\n",
    "print(\"üîó Resolving video stream URL...\")\n",
    "\n",
    "proc = subprocess.run(\n",
    "    [\n",
    "        \"yt-dlp\",\n",
    "        \"-f\", \"best[ext=mp4]/best\",\n",
    "        \"-g\",\n",
    "        \"--no-playlist\",\n",
    "        VIDEO_URL\n",
    "    ],\n",
    "    capture_output=True,\n",
    "    text=True,\n",
    "    check=True\n",
    ")\n",
    "\n",
    "STREAM_URL = proc.stdout.strip().split(\"\\n\")[0]\n",
    "\n",
    "if not STREAM_URL.startswith(\"http\"):\n",
    "    print(\"‚ùå Failed to resolve video stream.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# ----------------------------\n",
    "# STEP 1: PoseLandmarker\n",
    "# ----------------------------\n",
    "base_options = python.BaseOptions(model_asset_path=MODEL_PATH)\n",
    "options = vision.PoseLandmarkerOptions(\n",
    "    base_options=base_options,\n",
    "    running_mode=vision.RunningMode.VIDEO,\n",
    "    output_segmentation_masks=False\n",
    ")\n",
    "detector = vision.PoseLandmarker.create_from_options(options)\n",
    "\n",
    "# ----------------------------\n",
    "# STEP 2: Video I/O (stream URL)\n",
    "# ----------------------------\n",
    "cap = cv2.VideoCapture(STREAM_URL)\n",
    "if not cap.isOpened():\n",
    "    print(\"‚ùå Could not open video stream.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "width  = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fps    = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "if fps <= 0:\n",
    "    fps = 30.0  # fallback (important!)\n",
    "\n",
    "fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "writer = cv2.VideoWriter(OUT_VIDEO, fourcc, fps, (width, height))\n",
    "\n",
    "# ----------------------------\n",
    "# STEP 3: CSV writer\n",
    "# ----------------------------\n",
    "csv_file = open(OUT_CSV, \"w\", newline=\"\")\n",
    "csv_writer = csv.writer(csv_file)\n",
    "\n",
    "csv_writer.writerow([\n",
    "    \"frame\",\n",
    "    \"timestamp_ms\",\n",
    "    \"landmark_id\",\n",
    "    \"x_norm\",\n",
    "    \"y_norm\",\n",
    "    \"z_norm\",\n",
    "    \"visibility\",\n",
    "    \"x_px\",\n",
    "    \"y_px\"\n",
    "])\n",
    "\n",
    "print(\"‚ñ∂ Processing stream...\")\n",
    "\n",
    "# ----------------------------\n",
    "# STEP 4: Frame loop\n",
    "# ----------------------------\n",
    "frame_idx = 0\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=frame_rgb)\n",
    "\n",
    "    timestamp_ms = int((frame_idx / fps) * 1000)\n",
    "\n",
    "    result = detector.detect_for_video(mp_image, timestamp_ms)\n",
    "\n",
    "    annotated_rgb = np.copy(frame_rgb)\n",
    "\n",
    "    if result.pose_landmarks:\n",
    "        pose_landmarks = result.pose_landmarks[0]\n",
    "\n",
    "        # ----- EXPORT LANDMARKS -----\n",
    "        for lm_id, lm in enumerate(pose_landmarks):\n",
    "            x_px = int(lm.x * width)\n",
    "            y_px = int(lm.y * height)\n",
    "\n",
    "            csv_writer.writerow([\n",
    "                frame_idx,\n",
    "                timestamp_ms,\n",
    "                lm_id,\n",
    "                lm.x,\n",
    "                lm.y,\n",
    "                lm.z,\n",
    "                lm.visibility,\n",
    "                x_px,\n",
    "                y_px\n",
    "            ])\n",
    "\n",
    "        # ----- DRAW LANDMARKS -----\n",
    "        pose_proto = landmark_pb2.NormalizedLandmarkList()\n",
    "        pose_proto.landmark.extend([\n",
    "            landmark_pb2.NormalizedLandmark(x=lm.x, y=lm.y, z=lm.z)\n",
    "            for lm in pose_landmarks\n",
    "        ])\n",
    "\n",
    "        mp.solutions.drawing_utils.draw_landmarks(\n",
    "            annotated_rgb,\n",
    "            pose_proto,\n",
    "            mp.solutions.pose.POSE_CONNECTIONS,\n",
    "            landmark_drawing_spec=mp.solutions.drawing_utils.DrawingSpec(\n",
    "                color=(0, 255, 0), thickness=2, circle_radius=2\n",
    "            ),\n",
    "            connection_drawing_spec=mp.solutions.drawing_utils.DrawingSpec(\n",
    "                color=(255, 0, 0), thickness=2\n",
    "            )\n",
    "        )\n",
    "\n",
    "    annotated_bgr = cv2.cvtColor(annotated_rgb, cv2.COLOR_RGB2BGR)\n",
    "    writer.write(annotated_bgr)\n",
    "\n",
    "    if frame_idx % 30 == 0:\n",
    "        print(f\"\\r‚è≥ Frame {frame_idx}\", end=\"\")\n",
    "\n",
    "    frame_idx += 1\n",
    "\n",
    "# ----------------------------\n",
    "# STEP 5: Cleanup\n",
    "# ----------------------------\n",
    "cap.release()\n",
    "writer.release()\n",
    "csv_file.close()\n",
    "\n",
    "print(\"\\n‚úÖ Done!\")\n",
    "print(f\"üé¨ Video: {OUT_VIDEO}\")\n",
    "print(f\"üìä CSV:   {OUT_CSV}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
