{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "68779bc9",
      "metadata": {
        "id": "68779bc9"
      },
      "source": [
        "# Test OpenPose heavy model\n",
        "\n",
        "Computational heavy"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test the ST-GCN"
      ],
      "metadata": {
        "id": "Vz5XD9NK2w5N"
      },
      "id": "Vz5XD9NK2w5N"
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if GPU is available\n",
        "import torch\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
      ],
      "metadata": {
        "id": "VI9ATKos2zjv",
        "outputId": "58d4dfbf-24bd-49b3-af42-592408722a37",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "VI9ATKos2zjv",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch version: 2.9.0+cpu\n",
            "CUDA available: False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install dependencies\n",
        "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
        "!pip install numpy\n",
        "!pip install matplotlib\n",
        "!pip install opencv-python\n",
        "!pip install tqdm\n",
        "!pip install scikit-learn\n",
        "!pip install pandas"
      ],
      "metadata": {
        "id": "fDcpTqHJ3NQS"
      },
      "id": "fDcpTqHJ3NQS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Clone the ST-GCN repository\n",
        "!git clone https://github.com/yysijie/st-gcn.git\n",
        "%cd st-gcn"
      ],
      "metadata": {
        "id": "ACuO9ZP23Ooq"
      },
      "id": "ACuO9ZP23Ooq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install additional dependencies required by ST-GCN\n",
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "JeOJCrta3QH0"
      },
      "id": "JeOJCrta3QH0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download pre-trained model\n",
        "!mkdir -p checkpoints\n",
        "!wget -P checkpoints https://github.com/yysijie/st-gcn/raw/master/checkpoints/kinetics-st-gcn.pt"
      ],
      "metadata": {
        "id": "bnvnMQfr3Uwi"
      },
      "id": "bnvnMQfr3Uwi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a directory for sample data\n",
        "!mkdir -p data/sample\n",
        "\n",
        "# Download sample videos (you can replace these with your own videos)\n",
        "!wget -P data/sample https://github.com/yysijie/st-gcn/raw/master/resource/media/skateboarding.mp4\n",
        "!wget -P data/sample https://github.com/yysijie/st-gcn/raw/master/resource/media/walking.mp4"
      ],
      "metadata": {
        "id": "T-C2FPjY3Xso"
      },
      "id": "T-C2FPjY3Xso",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "from tqdm import tqdm\n",
        "import json\n",
        "from IPython.display import HTML\n",
        "from base64 import b64encode"
      ],
      "metadata": {
        "id": "6k5IIuG43YQv"
      },
      "id": "6k5IIuG43YQv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import ST-GCN modules\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "from net.st_gcn import Model\n",
        "from utils import *\n",
        "\n",
        "# Load the model\n",
        "def load_model():\n",
        "    num_class = 400  # Number of classes in Kinetics dataset\n",
        "    graph_args = {'layout': 'openpose', 'strategy': 'spatial'}\n",
        "    model = Model(num_class, graph_args, edge_importance_weighting=True)\n",
        "\n",
        "    # Load the pre-trained weights\n",
        "    weights_path = 'checkpoints/kinetics-st-gcn.pt'\n",
        "    weights = torch.load(weights_path)\n",
        "\n",
        "    if 'state_dict' in weights:\n",
        "        # Old format weights\n",
        "        model.load_state_dict(weights['state_dict'])\n",
        "    else:\n",
        "        # New format weights\n",
        "        model.load_state_dict(weights)\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    # Move model to GPU if available\n",
        "    if torch.cuda.is_available():\n",
        "        model = model.cuda()\n",
        "\n",
        "    return model\n",
        "\n",
        "model = load_model()\n",
        "print(\"Model loaded successfully!\")"
      ],
      "metadata": {
        "id": "hn4JOVCW3aZ5"
      },
      "id": "hn4JOVCW3aZ5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define utility functions for processing\n",
        "def load_json(file_path):\n",
        "    with open(file_path, 'r') as f:\n",
        "        return json.load(f)\n",
        "\n",
        "def process_video(video_path, output_path):\n",
        "    # Extract frames from video\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
        "    frames = []\n",
        "\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        frames.append(frame)\n",
        "\n",
        "    cap.release()\n",
        "\n",
        "    # Here you would normally extract keypoints using OpenPose or similar\n",
        "    # For this example, we'll simulate with random keypoints\n",
        "    num_frames = len(frames)\n",
        "    num_joints = 18  # OpenPose format\n",
        "    num_coords = 3  # x, y, confidence\n",
        "\n",
        "    # Generate random keypoints (replace with actual keypoint extraction)\n",
        "    keypoints = np.random.rand(num_frames, num_joints, num_coords)\n",
        "\n",
        "    # Save keypoints\n",
        "    np.save(output_path, keypoints)\n",
        "\n",
        "    return keypoints\n",
        "\n",
        "def load_keypoints(keypoints_path):\n",
        "    return np.load(keypoints_path)\n",
        "\n",
        "def preprocess_keypoints(keypoints):\n",
        "    # Normalize keypoints\n",
        "    # This is a simplified version - actual preprocessing would be more complex\n",
        "    keypoints = keypoints.reshape(keypoints.shape[0], -1)\n",
        "    mean = np.mean(keypoints, axis=0)\n",
        "    std = np.std(keypoints, axis=0)\n",
        "    keypoints = (keypoints - mean) / (std + 1e-9)\n",
        "\n",
        "    # Reshape back to original format\n",
        "    num_frames = keypoints.shape[0]\n",
        "    num_joints = 18\n",
        "    num_coords = 3\n",
        "    keypoints = keypoints.reshape(num_frames, num_joints, num_coords)\n",
        "\n",
        "    return keypoints\n",
        "\n",
        "def predict_action(model, keypoints):\n",
        "    # Preprocess keypoints\n",
        "    keypoints = preprocess_keypoints(keypoints)\n",
        "\n",
        "    # Convert to tensor\n",
        "    keypoints = torch.tensor(keypoints, dtype=torch.float32)\n",
        "\n",
        "    # Add batch dimension\n",
        "    keypoints = keypoints.unsqueeze(0)\n",
        "\n",
        "    # Move to GPU if available\n",
        "    if torch.cuda.is_available():\n",
        "        keypoints = keypoints.cuda()\n",
        "\n",
        "    # Make prediction\n",
        "    with torch.no_grad():\n",
        "        output = model(keypoints)\n",
        "\n",
        "    # Get predicted class\n",
        "    _, predicted = torch.max(output, 1)\n",
        "\n",
        "    return predicted.item()\n",
        "\n",
        "# Load action labels\n",
        "def load_labels():\n",
        "    # This is a simplified version - actual labels would come from the dataset\n",
        "    labels = [f\"action_{i}\" for i in range(400)]\n",
        "    return labels\n",
        "\n",
        "labels = load_labels()"
      ],
      "metadata": {
        "id": "mfCirp1c3c4C"
      },
      "id": "mfCirp1c3c4C",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Process sample videos and extract keypoints\n",
        "video_paths = ['data/sample/skateboarding.mp4', 'data/sample/walking.mp4']\n",
        "keypoints_paths = []\n",
        "\n",
        "for video_path in video_paths:\n",
        "    video_name = os.path.basename(video_path).split('.')[0]\n",
        "    keypoints_path = f'data/sample/{video_name}_keypoints.npy'\n",
        "\n",
        "    if not os.path.exists(keypoints_path):\n",
        "        print(f\"Processing {video_path}...\")\n",
        "        process_video(video_path, keypoints_path)\n",
        "        print(f\"Keypoints saved to {keypoints_path}\")\n",
        "    else:\n",
        "        print(f\"Keypoints already exist at {keypoints_path}\")\n",
        "\n",
        "    keypoints_paths.append(keypoints_path)"
      ],
      "metadata": {
        "id": "WDKjDKuW3gzw"
      },
      "id": "WDKjDKuW3gzw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run inference on sample videos\n",
        "for i, (video_path, keypoints_path) in enumerate(zip(video_paths, keypoints_paths)):\n",
        "    print(f\"\\nProcessing video: {video_path}\")\n",
        "\n",
        "    # Load keypoints\n",
        "    keypoints = load_keypoints(keypoints_path)\n",
        "    print(f\"Keypoints shape: {keypoints.shape}\")\n",
        "\n",
        "    # Make prediction\n",
        "    predicted_class = predict_action(model, keypoints)\n",
        "    print(f\"Predicted action: {labels[predicted_class]}\")\n",
        "\n",
        "    # Display video\n",
        "    print(f\"Displaying video: {video_path}\")\n",
        "    mp4 = open(video_path,'rb').read()\n",
        "    data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
        "    display(HTML(f\"\"\"\n",
        "    <video width=400 controls>\n",
        "          <source src=\"{data_url}\" type=\"video/mp4\">\n",
        "    </video>\n",
        "    \"\"\"))"
      ],
      "metadata": {
        "id": "BNz9JwLk3kKK"
      },
      "id": "BNz9JwLk3kKK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize keypoints on frames\n",
        "def visualize_keypoints(video_path, keypoints_path):\n",
        "    # Load video\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "    # Load keypoints\n",
        "    keypoints = load_keypoints(keypoints_path)\n",
        "\n",
        "    # Get frame info\n",
        "    num_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
        "\n",
        "    # Create output video\n",
        "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    output_path = video_path.replace('.mp4', '_keypoints.mp4')\n",
        "    out = cv2.VideoWriter(output_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (width, height))\n",
        "\n",
        "    # Define skeleton connections (OpenPose format)\n",
        "    skeleton = [\n",
        "        (1, 2), (1, 5), (2, 3), (3, 4), (5, 6), (6, 7),\n",
        "        (1, 8), (8, 9), (9, 10), (1, 11), (11, 12), (12, 13),\n",
        "        (1, 0), (0, 14), (14, 16), (0, 15), (15, 17)\n",
        "    ]\n",
        "\n",
        "    # Process each frame\n",
        "    frame_idx = 0\n",
        "    while cap.isOpened() and frame_idx < len(keypoints):\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        # Get keypoints for this frame\n",
        "        frame_keypoints = keypoints[frame_idx]\n",
        "\n",
        "        # Draw keypoints\n",
        "        for kp in frame_keypoints:\n",
        "            if kp[2] > 0.3:  # Confidence threshold\n",
        "                cv2.circle(frame, (int(kp[0]), int(kp[1])), 3, (0, 255, 0), -1)\n",
        "\n",
        "        # Draw skeleton\n",
        "        for connection in skeleton:\n",
        "            kp1 = frame_keypoints[connection[0]]\n",
        "            kp2 = frame_keypoints[connection[1]]\n",
        "            if kp1[2] > 0.3 and kp2[2] > 0.3:  # Confidence threshold\n",
        "                cv2.line(frame, (int(kp1[0]), int(kp1[1])), (int(kp2[0]), int(kp2[1])), (255, 0, 0), 2)\n",
        "\n",
        "        # Write frame to output video\n",
        "        out.write(frame)\n",
        "\n",
        "        frame_idx += 1\n",
        "\n",
        "    # Release resources\n",
        "    cap.release()\n",
        "    out.release()\n",
        "\n",
        "    return output_path\n",
        "\n",
        "# Visualize keypoints for each video\n",
        "for video_path, keypoints_path in zip(video_paths, keypoints_paths):\n",
        "    print(f\"\\nVisualizing keypoints for {video_path}\")\n",
        "    output_path = visualize_keypoints(video_path, keypoints_path)\n",
        "    print(f\"Output video saved to {output_path}\")\n",
        "\n",
        "    # Display video with keypoints\n",
        "    mp4 = open(output_path,'rb').read()\n",
        "    data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
        "    display(HTML(f\"\"\"\n",
        "    <video width=400 controls>\n",
        "          <source src=\"{data_url}\" type=\"video/mp4\">\n",
        "    </video>\n",
        "    \"\"\"))"
      ],
      "metadata": {
        "id": "kgeSvxa23mBZ"
      },
      "id": "kgeSvxa23mBZ",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}